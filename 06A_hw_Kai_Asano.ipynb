{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1GX1jvF6Ss1jQcb1uOYPX0ynUNQUwYrRq",
      "authorship_tag": "ABX9TyMXYM+7dlc3lbxGEUlUNhLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guest838/ML_Course-3.0/blob/main/06A_hw_Kai_Asano.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJca6WD_zI7b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fil = pd.read_csv(\"drive/MyDrive/filt/algos.csv\")\n",
        "fil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "-v2DDXrJJyzm",
        "outputId": "58c40905-527d-45b5-e2a4-9ff653e32481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     -1.0    -0.5     0.0  -0.5.1     0.5   0.5.1   0.0.1   0.5.2     1.0\n",
              "0  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb231d65-b8c2-4741-b696-9a2869719dfe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-1.0</th>\n",
              "      <th>-0.5</th>\n",
              "      <th>0.0</th>\n",
              "      <th>-0.5.1</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.5.1</th>\n",
              "      <th>0.0.1</th>\n",
              "      <th>0.5.2</th>\n",
              "      <th>1.0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb231d65-b8c2-4741-b696-9a2869719dfe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb231d65-b8c2-4741-b696-9a2869719dfe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb231d65-b8c2-4741-b696-9a2869719dfe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "fil",
              "summary": "{\n  \"name\": \"fil\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"-1.0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"-0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"-0.5.1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.5.1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.0.1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0.5.2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1.0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0625,\n        \"max\": 0.0625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    self.filt2.weight.data = ([[-1.0, -0.5, 0.0],\n",
        "      [-0.5, 0.5, 0.5],\n",
        "      [0.0, 0.5, 1.0]])\n",
        "    self.filt1.weight.data = ([[0.0625, 0.0625, 0.0625],\n",
        "      [0.0625, 0.0625, 0.0625],\n",
        "      [0.0625, 0.0625, 0.0625]])"
      ],
      "metadata": {
        "id": "qTYliK2UZmGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.filt1 = nn.Conv2d(1, 1, 3, padding=1, bias=False)\n",
        "      kernel = torch.FloatTensor([[[[0.0625, 0.0625, 0.0625],\n",
        "                                     [0.0625, 0.0625, 0.0625],\n",
        "                                     [0.0625, 0.0625, 0.0625]]]])\n",
        "      self.filt1.weight.data = nn.Parameter(kernel)\n",
        "      self.filt2 = nn.Conv2d(1, 1, 3, padding=1, bias=False)\n",
        "      self.filt3 = nn.Conv2d(1, 1, 3, padding=1, bias=False)\n",
        "      kernel3 = torch.FloatTensor([[[[-1.0, -0.5, 0.0],\n",
        "      [-0.5, 0.5, 0.5],\n",
        "      [0.0, 0.5, 1.0]]]])\n",
        "      self.filt3.weight.data = nn.Parameter(kernel3)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.filt1(x)\n",
        "        x = self.filt2(x)\n",
        "        x = self.filt3(x)\n",
        "        x = x.flatten()\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPGqgXBQIDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCats(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "      self.transform = transform\n",
        "      self.labels = [None] * 1000\n",
        "      self.data= [None] * 1000\n",
        "      for i in range(1, 1001):\n",
        "        label = np.loadtxt(f\"drive/MyDrive/filt/{i}.txt\")\n",
        "        label = np.array(label, dtype = np.float32).reshape(4096)\n",
        "        label = torch.from_numpy(label)\n",
        "        self.labels[i-1] = label\n",
        "        image =  torchvision.io.read_image(f\"drive/MyDrive/filt/{i}.png\")\n",
        "        image = np.array(image, dtype = np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "        self.data[i-1] = image\n",
        "        print(i)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
        "        # so you can convert numpy ndarray shape to tensor in PyTorch (C, H, W) --> (H, W, C)\n",
        "        image = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "GRQOb0rneCVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = DatasetCats(transform=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieKdt1AtgERT",
        "outputId": "304d45df-33d6-42c7-9772-c713b636d3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, lab = full_dataset.__getitem__(0)\n",
        "\n",
        "print(img.shape)\n",
        "print(type(img))\n",
        "print(lab.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZCe5eBshXrm",
        "outputId": "939005e2-5480-4c65-85b0-e946966593fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 64])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(full_dataset, batch_size=8, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "e-vADIQuiMFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newNet=Net()\n",
        "newNet.filt1.requires_grad_(False)\n",
        "newNet.filt3.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k20yvc6jDRI",
        "outputId": "794107c7-05e4-4909-8290-c2761fa1dbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchsummary\n",
        "torchsummary.summary(newNet, (1,64,64))"
      ],
      "metadata": {
        "id": "RA_Z0wRIowX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    torch.device(f\"cuda:{0}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "newNet.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q9MidBEj1A_",
        "outputId": "f6d26d83-6dca-4832-c8af-432d5ba0a57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (filt1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (filt2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (filt3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(newNet.parameters(), lr=1e-4)\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        outputs = newNet(images)\n",
        "        loss = loss_fn(outputs, labels.reshape(32768))\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmS4G6Q5ifhh",
        "outputId": "01ca70aa-99d7-43fb-c4cd-815e393dde65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Loss: 17153.538765625\n",
            "Epoch 2/1000, Loss: 15844.9241484375\n",
            "Epoch 3/1000, Loss: 14614.09039453125\n",
            "Epoch 4/1000, Loss: 13457.5758359375\n",
            "Epoch 5/1000, Loss: 12370.257578125\n",
            "Epoch 6/1000, Loss: 11347.69204296875\n",
            "Epoch 7/1000, Loss: 10388.53192578125\n",
            "Epoch 8/1000, Loss: 9490.85166796875\n",
            "Epoch 9/1000, Loss: 8651.20825390625\n",
            "Epoch 10/1000, Loss: 7867.25477734375\n",
            "Epoch 11/1000, Loss: 7137.14669921875\n",
            "Epoch 12/1000, Loss: 6457.72721484375\n",
            "Epoch 13/1000, Loss: 5827.57997265625\n",
            "Epoch 14/1000, Loss: 5243.35338671875\n",
            "Epoch 15/1000, Loss: 4703.848654296875\n",
            "Epoch 16/1000, Loss: 4206.773046875\n",
            "Epoch 17/1000, Loss: 3750.07569921875\n",
            "Epoch 18/1000, Loss: 3332.00990234375\n",
            "Epoch 19/1000, Loss: 2950.0762841796877\n",
            "Epoch 20/1000, Loss: 2602.977619140625\n",
            "Epoch 21/1000, Loss: 2289.0684326171877\n",
            "Epoch 22/1000, Loss: 2005.64197265625\n",
            "Epoch 23/1000, Loss: 1751.520107421875\n",
            "Epoch 24/1000, Loss: 1524.7295263671874\n",
            "Epoch 25/1000, Loss: 1323.6061225585938\n",
            "Epoch 26/1000, Loss: 1146.1124462890625\n",
            "Epoch 27/1000, Loss: 990.8541674804687\n",
            "Epoch 28/1000, Loss: 855.8478920898438\n",
            "Epoch 29/1000, Loss: 739.2977360839843\n",
            "Epoch 30/1000, Loss: 639.6122111816406\n",
            "Epoch 31/1000, Loss: 555.1060500488281\n",
            "Epoch 32/1000, Loss: 484.1438103027344\n",
            "Epoch 33/1000, Loss: 425.0794846191406\n",
            "Epoch 34/1000, Loss: 376.45619482421876\n",
            "Epoch 35/1000, Loss: 336.8179364013672\n",
            "Epoch 36/1000, Loss: 304.7442618408203\n",
            "Epoch 37/1000, Loss: 279.02971948242185\n",
            "Epoch 38/1000, Loss: 258.50490270996096\n",
            "Epoch 39/1000, Loss: 242.1645848388672\n",
            "Epoch 40/1000, Loss: 229.07780139160155\n",
            "Epoch 41/1000, Loss: 218.49882189941405\n",
            "Epoch 42/1000, Loss: 209.75171875\n",
            "Epoch 43/1000, Loss: 202.3265732421875\n",
            "Epoch 44/1000, Loss: 195.8013076171875\n",
            "Epoch 45/1000, Loss: 189.85092639160158\n",
            "Epoch 46/1000, Loss: 184.24130767822265\n",
            "Epoch 47/1000, Loss: 178.80014239501952\n",
            "Epoch 48/1000, Loss: 173.4187032470703\n",
            "Epoch 49/1000, Loss: 168.0196596069336\n",
            "Epoch 50/1000, Loss: 162.562556640625\n",
            "Epoch 51/1000, Loss: 157.03164709472657\n",
            "Epoch 52/1000, Loss: 151.4122416381836\n",
            "Epoch 53/1000, Loss: 145.7128355102539\n",
            "Epoch 54/1000, Loss: 139.9412770385742\n",
            "Epoch 55/1000, Loss: 134.1128798828125\n",
            "Epoch 56/1000, Loss: 128.23985052490235\n",
            "Epoch 57/1000, Loss: 122.34358074951172\n",
            "Epoch 58/1000, Loss: 116.43931634521485\n",
            "Epoch 59/1000, Loss: 110.5511337890625\n",
            "Epoch 60/1000, Loss: 104.69372045898437\n",
            "Epoch 61/1000, Loss: 98.89676745605469\n",
            "Epoch 62/1000, Loss: 93.17490063476562\n",
            "Epoch 63/1000, Loss: 87.54900384521484\n",
            "Epoch 64/1000, Loss: 82.04132730102539\n",
            "Epoch 65/1000, Loss: 76.6746625366211\n",
            "Epoch 66/1000, Loss: 71.46195904541015\n",
            "Epoch 67/1000, Loss: 66.42440155029297\n",
            "Epoch 68/1000, Loss: 61.582840637207035\n",
            "Epoch 69/1000, Loss: 56.94519369506836\n",
            "Epoch 70/1000, Loss: 52.525021133422854\n",
            "Epoch 71/1000, Loss: 48.33327166748047\n",
            "Epoch 72/1000, Loss: 44.38507604980469\n",
            "Epoch 73/1000, Loss: 40.68087022399902\n",
            "Epoch 74/1000, Loss: 37.22291177368164\n",
            "Epoch 75/1000, Loss: 34.01971434020996\n",
            "Epoch 76/1000, Loss: 31.066441177368166\n",
            "Epoch 77/1000, Loss: 28.360973419189452\n",
            "Epoch 78/1000, Loss: 25.89679440307617\n",
            "Epoch 79/1000, Loss: 23.667998947143555\n",
            "Epoch 80/1000, Loss: 21.663576904296875\n",
            "Epoch 81/1000, Loss: 19.873614601135255\n",
            "Epoch 82/1000, Loss: 18.283036575317382\n",
            "Epoch 83/1000, Loss: 16.879061264038086\n",
            "Epoch 84/1000, Loss: 15.643839111328125\n",
            "Epoch 85/1000, Loss: 14.562943351745606\n",
            "Epoch 86/1000, Loss: 13.61892381286621\n",
            "Epoch 87/1000, Loss: 12.794256507873536\n",
            "Epoch 88/1000, Loss: 12.073675552368163\n",
            "Epoch 89/1000, Loss: 11.439919418334961\n",
            "Epoch 90/1000, Loss: 10.879544292449951\n",
            "Epoch 91/1000, Loss: 10.377779972076416\n",
            "Epoch 92/1000, Loss: 9.922483200073243\n",
            "Epoch 93/1000, Loss: 9.502848361968994\n",
            "Epoch 94/1000, Loss: 9.109250091552735\n",
            "Epoch 95/1000, Loss: 8.734999393463134\n",
            "Epoch 96/1000, Loss: 8.373562438964843\n",
            "Epoch 97/1000, Loss: 8.020379257202148\n",
            "Epoch 98/1000, Loss: 7.6725547370910645\n",
            "Epoch 99/1000, Loss: 7.328350261688232\n",
            "Epoch 100/1000, Loss: 6.985691226959228\n",
            "Epoch 101/1000, Loss: 6.644689594268799\n",
            "Epoch 102/1000, Loss: 6.3049694976806645\n",
            "Epoch 103/1000, Loss: 5.967302856445312\n",
            "Epoch 104/1000, Loss: 5.632235012054443\n",
            "Epoch 105/1000, Loss: 5.301156999588013\n",
            "Epoch 106/1000, Loss: 4.974521947860718\n",
            "Epoch 107/1000, Loss: 4.653976104736328\n",
            "Epoch 108/1000, Loss: 4.3399790725708005\n",
            "Epoch 109/1000, Loss: 4.034134704589844\n",
            "Epoch 110/1000, Loss: 3.736888608932495\n",
            "Epoch 111/1000, Loss: 3.4497587757110595\n",
            "Epoch 112/1000, Loss: 3.173369271278381\n",
            "Epoch 113/1000, Loss: 2.9085916957855225\n",
            "Epoch 114/1000, Loss: 2.6560117359161377\n",
            "Epoch 115/1000, Loss: 2.416299221992493\n",
            "Epoch 116/1000, Loss: 2.189872305870056\n",
            "Epoch 117/1000, Loss: 1.9769442682266236\n",
            "Epoch 118/1000, Loss: 1.7778023872375488\n",
            "Epoch 119/1000, Loss: 1.5925272574424745\n",
            "Epoch 120/1000, Loss: 1.4210068612098694\n",
            "Epoch 121/1000, Loss: 1.2631418838500976\n",
            "Epoch 122/1000, Loss: 1.1186945900917054\n",
            "Epoch 123/1000, Loss: 0.9872190136909484\n",
            "Epoch 124/1000, Loss: 0.8682798628807068\n",
            "Epoch 125/1000, Loss: 0.7613418369293213\n",
            "Epoch 126/1000, Loss: 0.6657161827087402\n",
            "Epoch 127/1000, Loss: 0.580832457780838\n",
            "Epoch 128/1000, Loss: 0.5058664174079895\n",
            "Epoch 129/1000, Loss: 0.4401169385910034\n",
            "Epoch 130/1000, Loss: 0.3828877294063568\n",
            "Epoch 131/1000, Loss: 0.3333822416067123\n",
            "Epoch 132/1000, Loss: 0.2908699027299881\n",
            "Epoch 133/1000, Loss: 0.25462607884407046\n",
            "Epoch 134/1000, Loss: 0.22387724769115447\n",
            "Epoch 135/1000, Loss: 0.19803039693832397\n",
            "Epoch 136/1000, Loss: 0.17636967241764068\n",
            "Epoch 137/1000, Loss: 0.15829205322265624\n",
            "Epoch 138/1000, Loss: 0.14325286942720414\n",
            "Epoch 139/1000, Loss: 0.13072204095125198\n",
            "Epoch 140/1000, Loss: 0.120250325858593\n",
            "Epoch 141/1000, Loss: 0.11143808993697167\n",
            "Epoch 142/1000, Loss: 0.10392482334375382\n",
            "Epoch 143/1000, Loss: 0.09743795177340507\n",
            "Epoch 144/1000, Loss: 0.09173514443635941\n",
            "Epoch 145/1000, Loss: 0.08663474169373513\n",
            "Epoch 146/1000, Loss: 0.08200423926115036\n",
            "Epoch 147/1000, Loss: 0.07772923144698143\n",
            "Epoch 148/1000, Loss: 0.07374856120347976\n",
            "Epoch 149/1000, Loss: 0.07001637509465218\n",
            "Epoch 150/1000, Loss: 0.06649635609984397\n",
            "Epoch 151/1000, Loss: 0.06317462286353111\n",
            "Epoch 152/1000, Loss: 0.0600365706384182\n",
            "Epoch 153/1000, Loss: 0.057077116578817366\n",
            "Epoch 154/1000, Loss: 0.054299229562282565\n",
            "Epoch 155/1000, Loss: 0.051689452603459356\n",
            "Epoch 156/1000, Loss: 0.04924801525473595\n",
            "Epoch 157/1000, Loss: 0.04696503187716007\n",
            "Epoch 158/1000, Loss: 0.04483444826304912\n",
            "Epoch 159/1000, Loss: 0.042846322908997536\n",
            "Epoch 160/1000, Loss: 0.04099556370079517\n",
            "Epoch 161/1000, Loss: 0.03925726380944252\n",
            "Epoch 162/1000, Loss: 0.03763935098052025\n",
            "Epoch 163/1000, Loss: 0.036114418879151344\n",
            "Epoch 164/1000, Loss: 0.03467756962776184\n",
            "Epoch 165/1000, Loss: 0.03332524174451828\n",
            "Epoch 166/1000, Loss: 0.03203725783526897\n",
            "Epoch 167/1000, Loss: 0.030811609372496603\n",
            "Epoch 168/1000, Loss: 0.029643460713326932\n",
            "Epoch 169/1000, Loss: 0.0285225844681263\n",
            "Epoch 170/1000, Loss: 0.027449306435883045\n",
            "Epoch 171/1000, Loss: 0.026407372057437897\n",
            "Epoch 172/1000, Loss: 0.025406484492123127\n",
            "Epoch 173/1000, Loss: 0.02443828385323286\n",
            "Epoch 174/1000, Loss: 0.023505476988852025\n",
            "Epoch 175/1000, Loss: 0.02259479297697544\n",
            "Epoch 176/1000, Loss: 0.021711838498711587\n",
            "Epoch 177/1000, Loss: 0.02085396223515272\n",
            "Epoch 178/1000, Loss: 0.020013986341655254\n",
            "Epoch 179/1000, Loss: 0.01920355010777712\n",
            "Epoch 180/1000, Loss: 0.018407651953399183\n",
            "Epoch 181/1000, Loss: 0.01762878342717886\n",
            "Epoch 182/1000, Loss: 0.016869328051805497\n",
            "Epoch 183/1000, Loss: 0.01611906995251775\n",
            "Epoch 184/1000, Loss: 0.015384372189641\n",
            "Epoch 185/1000, Loss: 0.014670155577361583\n",
            "Epoch 186/1000, Loss: 0.013956074833869934\n",
            "Epoch 187/1000, Loss: 0.013271085802465677\n",
            "Epoch 188/1000, Loss: 0.012586802963167429\n",
            "Epoch 189/1000, Loss: 0.011918630499392747\n",
            "Epoch 190/1000, Loss: 0.011265908818691968\n",
            "Epoch 191/1000, Loss: 0.01062396065145731\n",
            "Epoch 192/1000, Loss: 0.009999269124120474\n",
            "Epoch 193/1000, Loss: 0.00938805802166462\n",
            "Epoch 194/1000, Loss: 0.0088044140227139\n",
            "Epoch 195/1000, Loss: 0.00823748205974698\n",
            "Epoch 196/1000, Loss: 0.0077050462998449806\n",
            "Epoch 197/1000, Loss: 0.007192813381552696\n",
            "Epoch 198/1000, Loss: 0.006713396821171045\n",
            "Epoch 199/1000, Loss: 0.006249598573893309\n",
            "Epoch 200/1000, Loss: 0.005802179725840688\n",
            "Epoch 201/1000, Loss: 0.0053791846428066495\n",
            "Epoch 202/1000, Loss: 0.004972560340538621\n",
            "Epoch 203/1000, Loss: 0.004587676346302033\n",
            "Epoch 204/1000, Loss: 0.004226081160828471\n",
            "Epoch 205/1000, Loss: 0.0038836386147886514\n",
            "Epoch 206/1000, Loss: 0.003563962822780013\n",
            "Epoch 207/1000, Loss: 0.0032628570422530176\n",
            "Epoch 208/1000, Loss: 0.002981694422662258\n",
            "Epoch 209/1000, Loss: 0.0027173657519742847\n",
            "Epoch 210/1000, Loss: 0.002471224174834788\n",
            "Epoch 211/1000, Loss: 0.002238380086608231\n",
            "Epoch 212/1000, Loss: 0.002022046878002584\n",
            "Epoch 213/1000, Loss: 0.0018256585495546461\n",
            "Epoch 214/1000, Loss: 0.001646358230151236\n",
            "Epoch 215/1000, Loss: 0.0014854614036157727\n",
            "Epoch 216/1000, Loss: 0.0013401558678597211\n",
            "Epoch 217/1000, Loss: 0.0012057784143835305\n",
            "Epoch 218/1000, Loss: 0.0010837194975465536\n",
            "Epoch 219/1000, Loss: 0.000970265194075182\n",
            "Epoch 220/1000, Loss: 0.0008704691985622049\n",
            "Epoch 221/1000, Loss: 0.0007736623198725284\n",
            "Epoch 222/1000, Loss: 0.0006887616477906704\n",
            "Epoch 223/1000, Loss: 0.0006096027744933963\n",
            "Epoch 224/1000, Loss: 0.0005375946171116084\n",
            "Epoch 225/1000, Loss: 0.0004714757329784334\n",
            "Epoch 226/1000, Loss: 0.0004156976701924577\n",
            "Epoch 227/1000, Loss: 0.00036091015650890765\n",
            "Epoch 228/1000, Loss: 0.0003141289537306875\n",
            "Epoch 229/1000, Loss: 0.00027872586844023315\n",
            "Epoch 230/1000, Loss: 0.00023684687179047616\n",
            "Epoch 231/1000, Loss: 0.0002071817485266365\n",
            "Epoch 232/1000, Loss: 0.00017785480938619004\n",
            "Epoch 233/1000, Loss: 0.0001550012668594718\n",
            "Epoch 234/1000, Loss: 0.0001335505972383544\n",
            "Epoch 235/1000, Loss: 0.0001173393462668173\n",
            "Epoch 236/1000, Loss: 9.77022965089418e-05\n",
            "Epoch 237/1000, Loss: 8.35874892072752e-05\n",
            "Epoch 238/1000, Loss: 7.835594358039088e-05\n",
            "Epoch 239/1000, Loss: 6.146357167745009e-05\n",
            "Epoch 240/1000, Loss: 5.369823204819113e-05\n",
            "Epoch 241/1000, Loss: 4.530579167476389e-05\n",
            "Epoch 242/1000, Loss: 3.8730785046936946e-05\n",
            "Epoch 243/1000, Loss: 3.437143591872882e-05\n",
            "Epoch 244/1000, Loss: 2.7964775101281703e-05\n",
            "Epoch 245/1000, Loss: 4.0830150071997195e-05\n",
            "Epoch 246/1000, Loss: 2.0245329520548692e-05\n",
            "Epoch 247/1000, Loss: 1.7744505341397598e-05\n",
            "Epoch 248/1000, Loss: 1.5516349201789126e-05\n",
            "Epoch 249/1000, Loss: 1.3813624347676523e-05\n",
            "Epoch 250/1000, Loss: 1.1351746721629753e-05\n",
            "Epoch 251/1000, Loss: 1.0263211130222771e-05\n",
            "Epoch 252/1000, Loss: 2.9026077274465933e-05\n",
            "Epoch 253/1000, Loss: 1.5595080411003437e-05\n",
            "Epoch 254/1000, Loss: 6.251219574551215e-06\n",
            "Epoch 255/1000, Loss: 5.529024205316091e-06\n",
            "Epoch 256/1000, Loss: 4.8658148298272865e-06\n",
            "Epoch 257/1000, Loss: 4.230774982715957e-06\n",
            "Epoch 258/1000, Loss: 3.695458021866216e-06\n",
            "Epoch 259/1000, Loss: 3.2056604468380103e-06\n",
            "Epoch 260/1000, Loss: 2.897752776334528e-06\n",
            "Epoch 261/1000, Loss: 2.402038181571697e-06\n",
            "Epoch 262/1000, Loss: 1.3882265466236277e-05\n",
            "Epoch 263/1000, Loss: 3.4924932251669814e-06\n",
            "Epoch 264/1000, Loss: 1.3594321967502765e-06\n",
            "Epoch 265/1000, Loss: 1.1721807363755943e-06\n",
            "Epoch 266/1000, Loss: 9.89546372238692e-07\n",
            "Epoch 267/1000, Loss: 8.334884250871255e-07\n",
            "Epoch 268/1000, Loss: 4.671998766207253e-05\n",
            "Epoch 269/1000, Loss: 1.0638539684123316e-05\n",
            "Epoch 270/1000, Loss: 5.577420815825462e-07\n",
            "Epoch 271/1000, Loss: 5.067932986548839e-07\n",
            "Epoch 272/1000, Loss: 4.590540982007951e-07\n",
            "Epoch 273/1000, Loss: 4.103448127352749e-07\n",
            "Epoch 274/1000, Loss: 3.675332594639258e-07\n",
            "Epoch 275/1000, Loss: 3.243838477828831e-07\n",
            "Epoch 276/1000, Loss: 2.84393656102111e-07\n",
            "Epoch 277/1000, Loss: 2.485755821339808e-07\n",
            "Epoch 278/1000, Loss: 2.152555416614632e-07\n",
            "Epoch 279/1000, Loss: 1.848599989671129e-07\n",
            "Epoch 280/1000, Loss: 2.6250102044969026e-06\n",
            "Epoch 281/1000, Loss: 1.9126609075783563e-06\n",
            "Epoch 282/1000, Loss: 3.3290780101538077e-06\n",
            "Epoch 283/1000, Loss: 8.133395508991726e-06\n",
            "Epoch 284/1000, Loss: 5.702647778207392e-07\n",
            "Epoch 285/1000, Loss: 7.337399165407987e-08\n",
            "Epoch 286/1000, Loss: 5.918674767713128e-08\n",
            "Epoch 287/1000, Loss: 6.525343741259349e-08\n",
            "Epoch 288/1000, Loss: 8.467214507902554e-06\n",
            "Epoch 289/1000, Loss: 2.1813891248712025e-07\n",
            "Epoch 290/1000, Loss: 6.751518510839105e-08\n",
            "Epoch 291/1000, Loss: 6.213463515223339e-07\n",
            "Epoch 292/1000, Loss: 1.5690725299748465e-06\n",
            "Epoch 293/1000, Loss: 1.5382050456480555e-07\n",
            "Epoch 294/1000, Loss: 2.5004259933218975e-05\n",
            "Epoch 295/1000, Loss: 9.272799062195247e-06\n",
            "Epoch 296/1000, Loss: 1.1571191148362914e-08\n",
            "Epoch 297/1000, Loss: 1.0067286620341065e-08\n",
            "Epoch 298/1000, Loss: 9.03282468556199e-09\n",
            "Epoch 299/1000, Loss: 7.884239554556416e-09\n",
            "Epoch 300/1000, Loss: 6.8959201762197605e-09\n",
            "Epoch 301/1000, Loss: 6.098072955040834e-09\n",
            "Epoch 302/1000, Loss: 6.130754499622526e-09\n",
            "Epoch 303/1000, Loss: 1.4032264800434292e-07\n",
            "Epoch 304/1000, Loss: 8.140624909849237e-07\n",
            "Epoch 305/1000, Loss: 1.4747826944404352e-05\n",
            "Epoch 306/1000, Loss: 7.99446564414552e-09\n",
            "Epoch 307/1000, Loss: 2.5674073400594466e-09\n",
            "Epoch 308/1000, Loss: 2.1875605140664334e-09\n",
            "Epoch 309/1000, Loss: 1.8426541110727612e-09\n",
            "Epoch 310/1000, Loss: 3.62466324110855e-09\n",
            "Epoch 311/1000, Loss: 2.417209756711003e-08\n",
            "Epoch 312/1000, Loss: 2.1568173059691277e-05\n",
            "Epoch 313/1000, Loss: 4.5881685907112056e-07\n",
            "Epoch 314/1000, Loss: 9.031644918611192e-10\n",
            "Epoch 315/1000, Loss: 8.274085048221025e-10\n",
            "Epoch 316/1000, Loss: 7.971585442856366e-10\n",
            "Epoch 317/1000, Loss: 1.4521956372970379e-09\n",
            "Epoch 318/1000, Loss: 9.689632805010006e-09\n",
            "Epoch 319/1000, Loss: 1.5264448747887017e-08\n",
            "Epoch 320/1000, Loss: 8.889945716528125e-07\n",
            "Epoch 321/1000, Loss: 4.344724691250246e-06\n",
            "Epoch 322/1000, Loss: 3.870574482458622e-05\n",
            "Epoch 323/1000, Loss: 9.10036364907274e-08\n",
            "Epoch 324/1000, Loss: 2.947729784708031e-10\n",
            "Epoch 325/1000, Loss: 2.622240277139198e-10\n",
            "Epoch 326/1000, Loss: 2.6204240710647754e-10\n",
            "Epoch 327/1000, Loss: 4.515762254131417e-10\n",
            "Epoch 328/1000, Loss: 3.8788599354688813e-10\n",
            "Epoch 329/1000, Loss: 3.8050627093610954e-10\n",
            "Epoch 330/1000, Loss: 3.5068754922207293e-10\n",
            "Epoch 331/1000, Loss: 2.678360293550952e-10\n",
            "Epoch 332/1000, Loss: 2.6342616255004004e-10\n",
            "Epoch 333/1000, Loss: 3.525069666165948e-10\n",
            "Epoch 334/1000, Loss: 3.452525678242502e-07\n",
            "Epoch 335/1000, Loss: 1.2756647370581576e-05\n",
            "Epoch 336/1000, Loss: 1.3200795210244198e-07\n",
            "Epoch 337/1000, Loss: 5.950222078388024e-10\n",
            "Epoch 338/1000, Loss: 2.5393020797181975e-10\n",
            "Epoch 339/1000, Loss: 3.1151924578809444e-10\n",
            "Epoch 340/1000, Loss: 3.5566398748354543e-10\n",
            "Epoch 341/1000, Loss: 1.949592195143879e-05\n",
            "Epoch 342/1000, Loss: 6.127825913609942e-08\n",
            "Epoch 343/1000, Loss: 2.5172904805081943e-10\n",
            "Epoch 344/1000, Loss: 1.3616531430171718e-10\n",
            "Epoch 345/1000, Loss: 1.3488872258049512e-10\n",
            "Epoch 346/1000, Loss: 1.3115902816607238e-10\n",
            "Epoch 347/1000, Loss: 6.930092060208892e-10\n",
            "Epoch 348/1000, Loss: 4.862594696446232e-06\n",
            "Epoch 349/1000, Loss: 4.0564943798937494e-06\n",
            "Epoch 350/1000, Loss: 2.623861962702811e-10\n",
            "Epoch 351/1000, Loss: 1.2479764210926448e-10\n",
            "Epoch 352/1000, Loss: 1.2592621578821905e-10\n",
            "Epoch 353/1000, Loss: 2.8632069358760613e-05\n",
            "Epoch 354/1000, Loss: 6.354142102127547e-05\n",
            "Epoch 355/1000, Loss: 5.34945682451049e-09\n",
            "Epoch 356/1000, Loss: 1.1023833207390866e-10\n",
            "Epoch 357/1000, Loss: 1.094824036151909e-10\n",
            "Epoch 358/1000, Loss: 1.0948533318844156e-10\n",
            "Epoch 359/1000, Loss: 1.0961327392977615e-10\n",
            "Epoch 360/1000, Loss: 1.0907817329930403e-10\n",
            "Epoch 361/1000, Loss: 1.1206086272808058e-10\n",
            "Epoch 362/1000, Loss: 1.1155498186310453e-10\n",
            "Epoch 363/1000, Loss: 1.0894628382773775e-10\n",
            "Epoch 364/1000, Loss: 1.1114783199372979e-10\n",
            "Epoch 365/1000, Loss: 1.1425200835413207e-10\n",
            "Epoch 366/1000, Loss: 1.1156124962719005e-10\n",
            "Epoch 367/1000, Loss: 1.1048220593945502e-10\n",
            "Epoch 368/1000, Loss: 1.3494697254090583e-10\n",
            "Epoch 369/1000, Loss: 1.0793769023020872e-10\n",
            "Epoch 370/1000, Loss: 1.0834729269326715e-10\n",
            "Epoch 371/1000, Loss: 1.0996138116725618e-10\n",
            "Epoch 372/1000, Loss: 1.801294303160894e-10\n",
            "Epoch 373/1000, Loss: 1.2228179152584673e-10\n",
            "Epoch 374/1000, Loss: 1.238272029913112e-10\n",
            "Epoch 375/1000, Loss: 1.4464209097120318e-10\n",
            "Epoch 376/1000, Loss: 2.510151073926581e-05\n",
            "Epoch 377/1000, Loss: 2.0523879242173581e-07\n",
            "Epoch 378/1000, Loss: 4.1887431989628963e-10\n",
            "Epoch 379/1000, Loss: 8.127584477657379e-11\n",
            "Epoch 380/1000, Loss: 8.449362867701815e-11\n",
            "Epoch 381/1000, Loss: 8.625662337169437e-11\n",
            "Epoch 382/1000, Loss: 1.4953324539712655e-10\n",
            "Epoch 383/1000, Loss: 2.3217559791399723e-10\n",
            "Epoch 384/1000, Loss: 1.2628940501691054e-10\n",
            "Epoch 385/1000, Loss: 3.9725819521446717e-07\n",
            "Epoch 386/1000, Loss: 2.0903654722749465e-05\n",
            "Epoch 387/1000, Loss: 5.963160062910644e-08\n",
            "Epoch 388/1000, Loss: 2.096829796127686e-09\n",
            "Epoch 389/1000, Loss: 3.1679608104018265e-09\n",
            "Epoch 390/1000, Loss: 1.0459152971487029e-10\n",
            "Epoch 391/1000, Loss: 1.1481143896707734e-10\n",
            "Epoch 392/1000, Loss: 1.358614674096259e-10\n",
            "Epoch 393/1000, Loss: 2.391860099981713e-10\n",
            "Epoch 394/1000, Loss: 2.9382908545150244e-10\n",
            "Epoch 395/1000, Loss: 2.2340290056765967e-10\n",
            "Epoch 396/1000, Loss: 4.4624005866549906e-05\n",
            "Epoch 397/1000, Loss: 8.658775459469137e-09\n",
            "Epoch 398/1000, Loss: 1.4965141900691316e-10\n",
            "Epoch 399/1000, Loss: 1.1639028363319426e-10\n",
            "Epoch 400/1000, Loss: 1.1985871128938896e-10\n",
            "Epoch 401/1000, Loss: 1.2074671412731775e-10\n",
            "Epoch 402/1000, Loss: 1.2095597301531314e-10\n",
            "Epoch 403/1000, Loss: 1.156870057528181e-10\n",
            "Epoch 404/1000, Loss: 1.1668283844690209e-10\n",
            "Epoch 405/1000, Loss: 1.201582076060248e-10\n",
            "Epoch 406/1000, Loss: 2.1123464333250298e-10\n",
            "Epoch 407/1000, Loss: 1.2200599694422997e-10\n",
            "Epoch 408/1000, Loss: 3.8011951633437624e-07\n",
            "Epoch 409/1000, Loss: 2.6816343047755708e-05\n",
            "Epoch 410/1000, Loss: 1.3098295183033848e-08\n",
            "Epoch 411/1000, Loss: 2.135181910167816e-10\n",
            "Epoch 412/1000, Loss: 1.3693974448836245e-10\n",
            "Epoch 413/1000, Loss: 1.3605055448895342e-10\n",
            "Epoch 414/1000, Loss: 1.3315656527401742e-10\n",
            "Epoch 415/1000, Loss: 1.948042356869628e-10\n",
            "Epoch 416/1000, Loss: 1.1250535364437652e-10\n",
            "Epoch 417/1000, Loss: 1.1571244995511876e-10\n",
            "Epoch 418/1000, Loss: 1.117685522467049e-10\n",
            "Epoch 419/1000, Loss: 1.2270362875765882e-10\n",
            "Epoch 420/1000, Loss: 1.6336918876391772e-05\n",
            "Epoch 421/1000, Loss: 7.510430782953215e-09\n",
            "Epoch 422/1000, Loss: 1.0093932471255674e-09\n",
            "Epoch 423/1000, Loss: 5.460493212594564e-09\n",
            "Epoch 424/1000, Loss: 8.872706386275908e-11\n",
            "Epoch 425/1000, Loss: 1.3802351940524205e-07\n",
            "Epoch 426/1000, Loss: 3.153134557950743e-05\n",
            "Epoch 427/1000, Loss: 6.011682107676109e-09\n",
            "Epoch 428/1000, Loss: 6.000621917690019e-10\n",
            "Epoch 429/1000, Loss: 2.6955261038866252e-11\n",
            "Epoch 430/1000, Loss: 2.960403801266498e-11\n",
            "Epoch 431/1000, Loss: 1.8202848599963018e-11\n",
            "Epoch 432/1000, Loss: 1.497473618369827e-11\n",
            "Epoch 433/1000, Loss: 1.3489190632542947e-11\n",
            "Epoch 434/1000, Loss: 2.111939730736756e-11\n",
            "Epoch 435/1000, Loss: 1.314394713507333e-07\n",
            "Epoch 436/1000, Loss: 7.363121678723994e-07\n",
            "Epoch 437/1000, Loss: 1.930822428005996e-05\n",
            "Epoch 438/1000, Loss: 4.8693967962143426e-08\n",
            "Epoch 439/1000, Loss: 9.749848687290451e-10\n",
            "Epoch 440/1000, Loss: 2.149867470130351e-09\n",
            "Epoch 441/1000, Loss: 1.3458747774586223e-10\n",
            "Epoch 442/1000, Loss: 1.3439976431772748e-10\n",
            "Epoch 443/1000, Loss: 8.160398821677184e-10\n",
            "Epoch 444/1000, Loss: 4.787268010204571e-07\n",
            "Epoch 445/1000, Loss: 2.196096547319826e-05\n",
            "Epoch 446/1000, Loss: 1.954459640620687e-07\n",
            "Epoch 447/1000, Loss: 1.8902760493544333e-09\n",
            "Epoch 448/1000, Loss: 5.34713840094625e-10\n",
            "Epoch 449/1000, Loss: 2.3639090440674694e-10\n",
            "Epoch 450/1000, Loss: 1.9737939221942114e-10\n",
            "Epoch 451/1000, Loss: 1.8592332168276826e-10\n",
            "Epoch 452/1000, Loss: 4.686921459828853e-10\n",
            "Epoch 453/1000, Loss: 5.861488512615143e-11\n",
            "Epoch 454/1000, Loss: 5.5040320578355753e-05\n",
            "Epoch 455/1000, Loss: 7.0447936081586836e-06\n",
            "Epoch 456/1000, Loss: 1.66569948223394e-10\n",
            "Epoch 457/1000, Loss: 1.3667908266290496e-10\n",
            "Epoch 458/1000, Loss: 1.3530043696530215e-10\n",
            "Epoch 459/1000, Loss: 1.362019417805449e-10\n",
            "Epoch 460/1000, Loss: 1.3476412413782058e-10\n",
            "Epoch 461/1000, Loss: 1.3609323024077468e-10\n",
            "Epoch 462/1000, Loss: 1.3321544106714711e-10\n",
            "Epoch 463/1000, Loss: 1.1858177800805335e-10\n",
            "Epoch 464/1000, Loss: 1.2838054957997614e-10\n",
            "Epoch 465/1000, Loss: 1.766998098950978e-10\n",
            "Epoch 466/1000, Loss: 2.1234663571956247e-10\n",
            "Epoch 467/1000, Loss: 1.3889292971303036e-10\n",
            "Epoch 468/1000, Loss: 1.8539141571904948e-10\n",
            "Epoch 469/1000, Loss: 2.7946716157911667e-10\n",
            "Epoch 470/1000, Loss: 1.784876018096071e-10\n",
            "Epoch 471/1000, Loss: 2.01651196851671e-10\n",
            "Epoch 472/1000, Loss: 3.394398715706071e-05\n",
            "Epoch 473/1000, Loss: 2.323437811257989e-09\n",
            "Epoch 474/1000, Loss: 9.208991799576627e-10\n",
            "Epoch 475/1000, Loss: 5.8137281828962715e-11\n",
            "Epoch 476/1000, Loss: 1.042778564053215e-10\n",
            "Epoch 477/1000, Loss: 4.930410633652294e-10\n",
            "Epoch 478/1000, Loss: 3.606379028187057e-10\n",
            "Epoch 479/1000, Loss: 9.708661699581178e-11\n",
            "Epoch 480/1000, Loss: 3.538001805414304e-11\n",
            "Epoch 481/1000, Loss: 2.062995691914926e-09\n",
            "Epoch 482/1000, Loss: 1.0968039919012629e-05\n",
            "Epoch 483/1000, Loss: 1.6435636577172285e-06\n",
            "Epoch 484/1000, Loss: 3.6497829929149627e-10\n",
            "Epoch 485/1000, Loss: 1.6638282522574154e-10\n",
            "Epoch 486/1000, Loss: 2.276314687454928e-08\n",
            "Epoch 487/1000, Loss: 1.4271482658363977e-06\n",
            "Epoch 488/1000, Loss: 1.4864217575039084e-07\n",
            "Epoch 489/1000, Loss: 2.1871635798449417e-06\n",
            "Epoch 490/1000, Loss: 4.3624607956016525e-07\n",
            "Epoch 491/1000, Loss: 1.4497565724635564e-05\n",
            "Epoch 492/1000, Loss: 3.905394168612908e-09\n",
            "Epoch 493/1000, Loss: 7.313571820377618e-11\n",
            "Epoch 494/1000, Loss: 1.4245261836937084e-09\n",
            "Epoch 495/1000, Loss: 4.952316023487668e-08\n",
            "Epoch 496/1000, Loss: 2.167646915859023e-06\n",
            "Epoch 497/1000, Loss: 3.061858403347051e-07\n",
            "Epoch 498/1000, Loss: 1.0168110193928826e-05\n",
            "Epoch 499/1000, Loss: 4.191912781059681e-08\n",
            "Epoch 500/1000, Loss: 5.002458171643465e-09\n",
            "Epoch 501/1000, Loss: 2.7835238305351507e-08\n",
            "Epoch 502/1000, Loss: 2.874063967746032e-07\n",
            "Epoch 503/1000, Loss: 4.1571733757634895e-06\n",
            "Epoch 504/1000, Loss: 1.1689957129437435e-05\n",
            "Epoch 505/1000, Loss: 1.265895800101058e-09\n",
            "Epoch 506/1000, Loss: 4.909813450804101e-09\n",
            "Epoch 507/1000, Loss: 4.115159014905778e-08\n",
            "Epoch 508/1000, Loss: 3.9379657267718526e-08\n",
            "Epoch 509/1000, Loss: 7.165733871994373e-08\n",
            "Epoch 510/1000, Loss: 2.4640115702393993e-06\n",
            "Epoch 511/1000, Loss: 7.755236786197716e-06\n",
            "Epoch 512/1000, Loss: 5.583855662572024e-07\n",
            "Epoch 513/1000, Loss: 1.3857585068444144e-08\n",
            "Epoch 514/1000, Loss: 6.20036270705393e-08\n",
            "Epoch 515/1000, Loss: 1.6359846049115888e-05\n",
            "Epoch 516/1000, Loss: 8.378528490586846e-06\n",
            "Epoch 517/1000, Loss: 1.989446339256684e-08\n",
            "Epoch 518/1000, Loss: 2.2411850155723556e-09\n",
            "Epoch 519/1000, Loss: 1.3443296272008798e-09\n",
            "Epoch 520/1000, Loss: 4.352544034649796e-09\n",
            "Epoch 521/1000, Loss: 1.0605628750226265e-10\n",
            "Epoch 522/1000, Loss: 1.3549128125879551e-12\n",
            "Epoch 523/1000, Loss: 5.451476509793495e-13\n",
            "Epoch 524/1000, Loss: 5.4514765119619e-13\n",
            "Epoch 525/1000, Loss: 5.451476507625092e-13\n",
            "Epoch 526/1000, Loss: 1.5363046902347467e-12\n",
            "Epoch 527/1000, Loss: 3.1363544914387447e-12\n",
            "Epoch 528/1000, Loss: 6.322775536644898e-13\n",
            "Epoch 529/1000, Loss: 4.827202439855461e-13\n",
            "Epoch 530/1000, Loss: 4.987913623799578e-13\n",
            "Epoch 531/1000, Loss: 4.987913625967982e-13\n",
            "Epoch 532/1000, Loss: 4.987913623799578e-13\n",
            "Epoch 533/1000, Loss: 4.987913630304791e-13\n",
            "Epoch 534/1000, Loss: 4.987913621631174e-13\n",
            "Epoch 535/1000, Loss: 0.00018376598566205522\n",
            "Epoch 536/1000, Loss: 1.0943953709669274e-05\n",
            "Epoch 537/1000, Loss: 2.7640361888181533e-10\n",
            "Epoch 538/1000, Loss: 4.127146416288419e-11\n",
            "Epoch 539/1000, Loss: 4.628046040844325e-11\n",
            "Epoch 540/1000, Loss: 4.533638635340065e-11\n",
            "Epoch 541/1000, Loss: 4.645591725194187e-11\n",
            "Epoch 542/1000, Loss: 5.1421100946691387e-11\n",
            "Epoch 543/1000, Loss: 4.506628061007323e-11\n",
            "Epoch 544/1000, Loss: 4.832347798300329e-11\n",
            "Epoch 545/1000, Loss: 4.79348854498518e-11\n",
            "Epoch 546/1000, Loss: 4.817696944159344e-11\n",
            "Epoch 547/1000, Loss: 4.736545301808892e-11\n",
            "Epoch 548/1000, Loss: 4.729140695713952e-11\n",
            "Epoch 549/1000, Loss: 7.696781885446757e-11\n",
            "Epoch 550/1000, Loss: 1.2737598045764464e-10\n",
            "Epoch 551/1000, Loss: 1.4087792099548578e-10\n",
            "Epoch 552/1000, Loss: 1.1455757120693022e-10\n",
            "Epoch 553/1000, Loss: 2.3212035838438004e-09\n",
            "Epoch 554/1000, Loss: 1.6908621303102222e-09\n",
            "Epoch 555/1000, Loss: 8.22553464974174e-09\n",
            "Epoch 556/1000, Loss: 3.90067466008559e-07\n",
            "Epoch 557/1000, Loss: 4.942882852897457e-07\n",
            "Epoch 558/1000, Loss: 6.642066587062345e-06\n",
            "Epoch 559/1000, Loss: 1.769772360009636e-06\n",
            "Epoch 560/1000, Loss: 5.049761131303843e-09\n",
            "Epoch 561/1000, Loss: 2.8134011370528623e-08\n",
            "Epoch 562/1000, Loss: 1.323196613163269e-07\n",
            "Epoch 563/1000, Loss: 1.033905348530828e-05\n",
            "Epoch 564/1000, Loss: 2.326750620257334e-09\n",
            "Epoch 565/1000, Loss: 1.9461062683232e-11\n",
            "Epoch 566/1000, Loss: 1.0874676921110438e-11\n",
            "Epoch 567/1000, Loss: 8.224328213249365e-12\n",
            "Epoch 568/1000, Loss: 7.701354429179297e-12\n",
            "Epoch 569/1000, Loss: 6.3560169297899316e-12\n",
            "Epoch 570/1000, Loss: 5.342588992734987e-12\n",
            "Epoch 571/1000, Loss: 7.371079178184782e-09\n",
            "Epoch 572/1000, Loss: 9.431322243715634e-05\n",
            "Epoch 573/1000, Loss: 6.6760240678975794e-09\n",
            "Epoch 574/1000, Loss: 3.6837503172848506e-11\n",
            "Epoch 575/1000, Loss: 3.552530074280025e-11\n",
            "Epoch 576/1000, Loss: 3.5525300923211487e-11\n",
            "Epoch 577/1000, Loss: 3.552530088157812e-11\n",
            "Epoch 578/1000, Loss: 3.552530086770034e-11\n",
            "Epoch 579/1000, Loss: 3.5525300826066976e-11\n",
            "Epoch 580/1000, Loss: 3.552530077055582e-11\n",
            "Epoch 581/1000, Loss: 4.280996555861005e-11\n",
            "Epoch 582/1000, Loss: 3.341019359781594e-11\n",
            "Epoch 583/1000, Loss: 3.341019354230479e-11\n",
            "Epoch 584/1000, Loss: 3.34101934174047e-11\n",
            "Epoch 585/1000, Loss: 3.3410193375771336e-11\n",
            "Epoch 586/1000, Loss: 3.3410193556182575e-11\n",
            "Epoch 587/1000, Loss: 6.756382311956521e-11\n",
            "Epoch 588/1000, Loss: 9.446249976674537e-11\n",
            "Epoch 589/1000, Loss: 7.5573803591923725e-06\n",
            "Epoch 590/1000, Loss: 3.3244930615683456e-08\n",
            "Epoch 591/1000, Loss: 3.9041918819610056e-07\n",
            "Epoch 592/1000, Loss: 1.0635035978500573e-05\n",
            "Epoch 593/1000, Loss: 2.7675644105791264e-06\n",
            "Epoch 594/1000, Loss: 2.8396754637657896e-10\n",
            "Epoch 595/1000, Loss: 1.49308920192881e-11\n",
            "Epoch 596/1000, Loss: 2.0058771012243515e-11\n",
            "Epoch 597/1000, Loss: 1.7619779066335718e-11\n",
            "Epoch 598/1000, Loss: 1.3266511906390167e-11\n",
            "Epoch 599/1000, Loss: 1.1804324628883477e-11\n",
            "Epoch 600/1000, Loss: 1.1745835869908517e-11\n",
            "Epoch 601/1000, Loss: 1.2333157016486407e-11\n",
            "Epoch 602/1000, Loss: 1.2213309526915018e-11\n",
            "Epoch 603/1000, Loss: 2.9075735325285357e-05\n",
            "Epoch 604/1000, Loss: 1.191397733585342e-07\n",
            "Epoch 605/1000, Loss: 7.717962069442308e-10\n",
            "Epoch 606/1000, Loss: 2.2538673809802124e-09\n",
            "Epoch 607/1000, Loss: 9.360867224972313e-10\n",
            "Epoch 608/1000, Loss: 1.40878794804844e-09\n",
            "Epoch 609/1000, Loss: 1.6360291354294843e-10\n",
            "Epoch 610/1000, Loss: 4.22584700149331e-10\n",
            "Epoch 611/1000, Loss: 2.251967523581033e-08\n",
            "Epoch 612/1000, Loss: 4.505235585494205e-06\n",
            "Epoch 613/1000, Loss: 2.7399419181461937e-07\n",
            "Epoch 614/1000, Loss: 4.3373723809081443e-07\n",
            "Epoch 615/1000, Loss: 5.530219930431457e-05\n",
            "Epoch 616/1000, Loss: 1.1529679943317328e-06\n",
            "Epoch 617/1000, Loss: 1.5259933313499507e-10\n",
            "Epoch 618/1000, Loss: 9.509004956706479e-11\n",
            "Epoch 619/1000, Loss: 9.299050197572601e-11\n",
            "Epoch 620/1000, Loss: 1.275079816176472e-10\n",
            "Epoch 621/1000, Loss: 1.3960251213362973e-10\n",
            "Epoch 622/1000, Loss: 6.216719394547265e-10\n",
            "Epoch 623/1000, Loss: 3.5638338817534087e-10\n",
            "Epoch 624/1000, Loss: 8.848561249205634e-10\n",
            "Epoch 625/1000, Loss: 2.741769058145493e-07\n",
            "Epoch 626/1000, Loss: 9.060690644879799e-09\n",
            "Epoch 627/1000, Loss: 2.9193613298905687e-07\n",
            "Epoch 628/1000, Loss: 1.4741649996610761e-08\n",
            "Epoch 629/1000, Loss: 1.750473405139652e-05\n",
            "Epoch 630/1000, Loss: 2.003541008613141e-05\n",
            "Epoch 631/1000, Loss: 7.917834640713828e-10\n",
            "Epoch 632/1000, Loss: 9.278337106954914e-11\n",
            "Epoch 633/1000, Loss: 1.066711587671687e-10\n",
            "Epoch 634/1000, Loss: 8.010420093906844e-11\n",
            "Epoch 635/1000, Loss: 4.227880050222677e-11\n",
            "Epoch 636/1000, Loss: 8.722525735999076e-10\n",
            "Epoch 637/1000, Loss: 3.481799716147094e-10\n",
            "Epoch 638/1000, Loss: 2.9368935973612953e-10\n",
            "Epoch 639/1000, Loss: 2.4317883731000965e-12\n",
            "Epoch 640/1000, Loss: 1.0159830121300728e-12\n",
            "Epoch 641/1000, Loss: 1.428251612898418e-12\n",
            "Epoch 642/1000, Loss: 1.3270383789239227e-12\n",
            "Epoch 643/1000, Loss: 1.3258402435829819e-12\n",
            "Epoch 644/1000, Loss: 9.088814958943775e-05\n",
            "Epoch 645/1000, Loss: 1.0145142467116175e-07\n",
            "Epoch 646/1000, Loss: 1.2873320881512385e-11\n",
            "Epoch 647/1000, Loss: 7.807101076728885e-12\n",
            "Epoch 648/1000, Loss: 7.807101114892801e-12\n",
            "Epoch 649/1000, Loss: 8.716804139607959e-12\n",
            "Epoch 650/1000, Loss: 9.724687131951182e-12\n",
            "Epoch 651/1000, Loss: 9.724687083378925e-12\n",
            "Epoch 652/1000, Loss: 9.7246871111345e-12\n",
            "Epoch 653/1000, Loss: 9.7246871111345e-12\n",
            "Epoch 654/1000, Loss: 9.7246871111345e-12\n",
            "Epoch 655/1000, Loss: 9.724687090317818e-12\n",
            "Epoch 656/1000, Loss: 9.724687107665054e-12\n",
            "Epoch 657/1000, Loss: 9.72468714582897e-12\n",
            "Epoch 658/1000, Loss: 9.724687125012288e-12\n",
            "Epoch 659/1000, Loss: 9.724687076440031e-12\n",
            "Epoch 660/1000, Loss: 1.2239118000328952e-11\n",
            "Epoch 661/1000, Loss: 3.3287433053030834e-11\n",
            "Epoch 662/1000, Loss: 8.425328373578151e-07\n",
            "Epoch 663/1000, Loss: 1.2089154212779896e-05\n",
            "Epoch 664/1000, Loss: 2.936590564310837e-07\n",
            "Epoch 665/1000, Loss: 3.790268235795424e-08\n",
            "Epoch 666/1000, Loss: 4.569636008544542e-08\n",
            "Epoch 667/1000, Loss: 6.011893292439785e-06\n",
            "Epoch 668/1000, Loss: 1.2383951773974556e-05\n",
            "Epoch 669/1000, Loss: 1.8448040395857567e-09\n",
            "Epoch 670/1000, Loss: 2.577217160792955e-09\n",
            "Epoch 671/1000, Loss: 4.100293415243428e-09\n",
            "Epoch 672/1000, Loss: 2.8696314101517294e-08\n",
            "Epoch 673/1000, Loss: 3.9836128572223736e-08\n",
            "Epoch 674/1000, Loss: 1.0691318856475563e-06\n",
            "Epoch 675/1000, Loss: 3.4191605916022994e-06\n",
            "Epoch 676/1000, Loss: 5.17013669945915e-07\n",
            "Epoch 677/1000, Loss: 1.4640123202958044e-08\n",
            "Epoch 678/1000, Loss: 3.649189680710041e-08\n",
            "Epoch 679/1000, Loss: 2.28152684862426e-06\n",
            "Epoch 680/1000, Loss: 2.4790909391048997e-05\n",
            "Epoch 681/1000, Loss: 1.2935161473659474e-08\n",
            "Epoch 682/1000, Loss: 1.6503761393615868e-10\n",
            "Epoch 683/1000, Loss: 1.963719299391542e-10\n",
            "Epoch 684/1000, Loss: 8.239322918379699e-10\n",
            "Epoch 685/1000, Loss: 3.4586745216169668e-09\n",
            "Epoch 686/1000, Loss: 2.4417765114304047e-09\n",
            "Epoch 687/1000, Loss: 2.543499503238067e-10\n",
            "Epoch 688/1000, Loss: 3.342614027612179e-05\n",
            "Epoch 689/1000, Loss: 6.752198013605381e-07\n",
            "Epoch 690/1000, Loss: 7.635419360940699e-10\n",
            "Epoch 691/1000, Loss: 9.495907776768675e-11\n",
            "Epoch 692/1000, Loss: 1.0664327912413008e-10\n",
            "Epoch 693/1000, Loss: 1.6819883463803365e-10\n",
            "Epoch 694/1000, Loss: 2.051888817189429e-10\n",
            "Epoch 695/1000, Loss: 1.0789240953990032e-09\n",
            "Epoch 696/1000, Loss: 2.5500101579123415e-07\n",
            "Epoch 697/1000, Loss: 8.602518481519717e-07\n",
            "Epoch 698/1000, Loss: 7.237953417060794e-09\n",
            "Epoch 699/1000, Loss: 4.704010800713782e-05\n",
            "Epoch 700/1000, Loss: 5.1605222700215505e-05\n",
            "Epoch 701/1000, Loss: 1.3197290885669233e-09\n",
            "Epoch 702/1000, Loss: 7.09309555257942e-11\n",
            "Epoch 703/1000, Loss: 7.102988541785038e-11\n",
            "Epoch 704/1000, Loss: 7.181609162909552e-11\n",
            "Epoch 705/1000, Loss: 7.044793901012092e-11\n",
            "Epoch 706/1000, Loss: 7.012081440493923e-11\n",
            "Epoch 707/1000, Loss: 7.036936225390811e-11\n",
            "Epoch 708/1000, Loss: 7.026837092749539e-11\n",
            "Epoch 709/1000, Loss: 7.66212391034582e-11\n",
            "Epoch 710/1000, Loss: 7.020623740694453e-11\n",
            "Epoch 711/1000, Loss: 7.245393204846628e-11\n",
            "Epoch 712/1000, Loss: 1.0270188507965904e-10\n",
            "Epoch 713/1000, Loss: 9.158237609940834e-11\n",
            "Epoch 714/1000, Loss: 1.131412556143463e-10\n",
            "Epoch 715/1000, Loss: 9.186292498908343e-11\n",
            "Epoch 716/1000, Loss: 1.0401401212556394e-10\n",
            "Epoch 717/1000, Loss: 1.803245887610494e-10\n",
            "Epoch 718/1000, Loss: 3.376610175752182e-05\n",
            "Epoch 719/1000, Loss: 5.114790393652146e-07\n",
            "Epoch 720/1000, Loss: 1.1163855619250596e-09\n",
            "Epoch 721/1000, Loss: 2.726098490349327e-11\n",
            "Epoch 722/1000, Loss: 1.9183208389839113e-11\n",
            "Epoch 723/1000, Loss: 2.667433178504819e-11\n",
            "Epoch 724/1000, Loss: 1.276620560791697e-10\n",
            "Epoch 725/1000, Loss: 5.4182307680505914e-09\n",
            "Epoch 726/1000, Loss: 9.9860356462117e-09\n",
            "Epoch 727/1000, Loss: 1.3471219045962935e-08\n",
            "Epoch 728/1000, Loss: 1.2348677556228704e-08\n",
            "Epoch 729/1000, Loss: 1.1954912343607571e-05\n",
            "Epoch 730/1000, Loss: 1.1944876401215333e-08\n",
            "Epoch 731/1000, Loss: 6.540324429260558e-09\n",
            "Epoch 732/1000, Loss: 1.3782403953666033e-09\n",
            "Epoch 733/1000, Loss: 8.140028514067588e-07\n",
            "Epoch 734/1000, Loss: 2.4925818785632716e-06\n",
            "Epoch 735/1000, Loss: 1.0223056296509991e-07\n",
            "Epoch 736/1000, Loss: 4.6935948089021906e-08\n",
            "Epoch 737/1000, Loss: 1.1849313393162752e-05\n",
            "Epoch 738/1000, Loss: 3.896775524820484e-06\n",
            "Epoch 739/1000, Loss: 3.333943718134558e-07\n",
            "Epoch 740/1000, Loss: 8.923183344999864e-11\n",
            "Epoch 741/1000, Loss: 3.6387590091613297e-09\n",
            "Epoch 742/1000, Loss: 2.479724679482764e-10\n",
            "Epoch 743/1000, Loss: 1.6304476646578081e-10\n",
            "Epoch 744/1000, Loss: 1.395007005218496e-08\n",
            "Epoch 745/1000, Loss: 7.335665726316165e-06\n",
            "Epoch 746/1000, Loss: 2.97195929287275e-06\n",
            "Epoch 747/1000, Loss: 9.69631645887592e-07\n",
            "Epoch 748/1000, Loss: 9.277866174404182e-07\n",
            "Epoch 749/1000, Loss: 1.3537394481573983e-05\n",
            "Epoch 750/1000, Loss: 3.34856466530864e-07\n",
            "Epoch 751/1000, Loss: 1.274640185960907e-08\n",
            "Epoch 752/1000, Loss: 1.5031445055602167e-09\n",
            "Epoch 753/1000, Loss: 1.0553264139961938e-07\n",
            "Epoch 754/1000, Loss: 7.614641007904766e-07\n",
            "Epoch 755/1000, Loss: 5.059192822600034e-06\n",
            "Epoch 756/1000, Loss: 2.721778649764861e-06\n",
            "Epoch 757/1000, Loss: 3.9233441883768097e-07\n",
            "Epoch 758/1000, Loss: 2.814487871201088e-07\n",
            "Epoch 759/1000, Loss: 6.087217095443576e-06\n",
            "Epoch 760/1000, Loss: 7.602569961356664e-06\n",
            "Epoch 761/1000, Loss: 1.523810401965475e-08\n",
            "Epoch 762/1000, Loss: 2.7615789133239587e-10\n",
            "Epoch 763/1000, Loss: 6.000367505087922e-10\n",
            "Epoch 764/1000, Loss: 4.623116196617039e-09\n",
            "Epoch 765/1000, Loss: 6.417683622216464e-05\n",
            "Epoch 766/1000, Loss: 5.291664600844403e-07\n",
            "Epoch 767/1000, Loss: 2.7610907571418154e-10\n",
            "Epoch 768/1000, Loss: 1.6224776544504494e-10\n",
            "Epoch 769/1000, Loss: 3.21986993956358e-10\n",
            "Epoch 770/1000, Loss: 4.2186044305059234e-10\n",
            "Epoch 771/1000, Loss: 1.0982173091061975e-10\n",
            "Epoch 772/1000, Loss: 2.922267284058422e-12\n",
            "Epoch 773/1000, Loss: 2.6693959646884037e-12\n",
            "Epoch 774/1000, Loss: 2.6693959638210418e-12\n",
            "Epoch 775/1000, Loss: 2.66939596295368e-12\n",
            "Epoch 776/1000, Loss: 2.6693959612189565e-12\n",
            "Epoch 777/1000, Loss: 2.7304657042315217e-12\n",
            "Epoch 778/1000, Loss: 2.950132555780416e-12\n",
            "Epoch 779/1000, Loss: 2.3417062967531764e-12\n",
            "Epoch 780/1000, Loss: 2.1745248709320597e-12\n",
            "Epoch 781/1000, Loss: 2.1745248683299745e-12\n",
            "Epoch 782/1000, Loss: 2.312594087582054e-12\n",
            "Epoch 783/1000, Loss: 2.246232168150175e-12\n",
            "Epoch 784/1000, Loss: 2.3138503170774815e-12\n",
            "Epoch 785/1000, Loss: 2.3525951759612028e-12\n",
            "Epoch 786/1000, Loss: 2.3192695030108124e-12\n",
            "Epoch 787/1000, Loss: 2.2934620981865384e-12\n",
            "Epoch 788/1000, Loss: 2.3351996525713957e-12\n",
            "Epoch 789/1000, Loss: 1.5173808293189595e-11\n",
            "Epoch 790/1000, Loss: 0.0001186037070202456\n",
            "Epoch 791/1000, Loss: 9.108350637504481e-07\n",
            "Epoch 792/1000, Loss: 2.3114126837531224e-10\n",
            "Epoch 793/1000, Loss: 9.952894119980016e-11\n",
            "Epoch 794/1000, Loss: 1.0430453151188957e-10\n",
            "Epoch 795/1000, Loss: 1.0427600261042613e-10\n",
            "Epoch 796/1000, Loss: 1.0439079328738998e-10\n",
            "Epoch 797/1000, Loss: 1.0425730773144792e-10\n",
            "Epoch 798/1000, Loss: 1.0301362055042951e-10\n",
            "Epoch 799/1000, Loss: 1.0351769214045348e-10\n",
            "Epoch 800/1000, Loss: 1.0756977031345371e-10\n",
            "Epoch 801/1000, Loss: 1.2440367841426435e-10\n",
            "Epoch 802/1000, Loss: 1.2002449223391665e-10\n",
            "Epoch 803/1000, Loss: 1.1549146092248996e-10\n",
            "Epoch 804/1000, Loss: 9.376011306239107e-11\n",
            "Epoch 805/1000, Loss: 9.252176341734141e-11\n",
            "Epoch 806/1000, Loss: 1.1544863821877361e-10\n",
            "Epoch 807/1000, Loss: 5.6808962117793625e-11\n",
            "Epoch 808/1000, Loss: 9.134793577336708e-09\n",
            "Epoch 809/1000, Loss: 3.823943343705061e-06\n",
            "Epoch 810/1000, Loss: 2.2599005362249347e-06\n",
            "Epoch 811/1000, Loss: 1.288390661504102e-07\n",
            "Epoch 812/1000, Loss: 4.4659082743636614e-07\n",
            "Epoch 813/1000, Loss: 1.537558717939147e-05\n",
            "Epoch 814/1000, Loss: 3.159837061483817e-08\n",
            "Epoch 815/1000, Loss: 6.8195707611518055e-09\n",
            "Epoch 816/1000, Loss: 6.015540565778021e-07\n",
            "Epoch 817/1000, Loss: 3.956014448608381e-09\n",
            "Epoch 818/1000, Loss: 5.6935235756006004e-09\n",
            "Epoch 819/1000, Loss: 2.360623323882649e-08\n",
            "Epoch 820/1000, Loss: 2.532470531068087e-07\n",
            "Epoch 821/1000, Loss: 2.998719803147054e-05\n",
            "Epoch 822/1000, Loss: 2.3964878570342797e-08\n",
            "Epoch 823/1000, Loss: 1.1877954114725143e-09\n",
            "Epoch 824/1000, Loss: 3.3818240273014723e-09\n",
            "Epoch 825/1000, Loss: 1.584624408579316e-09\n",
            "Epoch 826/1000, Loss: 1.4767236297674735e-10\n",
            "Epoch 827/1000, Loss: 3.0340500181691432e-09\n",
            "Epoch 828/1000, Loss: 5.157913652853452e-10\n",
            "Epoch 829/1000, Loss: 1.563049072950129e-10\n",
            "Epoch 830/1000, Loss: 1.6865983129554783e-10\n",
            "Epoch 831/1000, Loss: 1.5154584409593497e-10\n",
            "Epoch 832/1000, Loss: 1.3417640720714275e-10\n",
            "Epoch 833/1000, Loss: 1.6953793569030041e-10\n",
            "Epoch 834/1000, Loss: 1.80669251892418e-10\n",
            "Epoch 835/1000, Loss: 6.95380262909584e-05\n",
            "Epoch 836/1000, Loss: 2.540531978869609e-07\n",
            "Epoch 837/1000, Loss: 3.305656311736183e-10\n",
            "Epoch 838/1000, Loss: 1.256003361455038e-10\n",
            "Epoch 839/1000, Loss: 1.2190893064478735e-10\n",
            "Epoch 840/1000, Loss: 1.1644452704850928e-10\n",
            "Epoch 841/1000, Loss: 1.488968186524886e-10\n",
            "Epoch 842/1000, Loss: 1.2741250710046613e-10\n",
            "Epoch 843/1000, Loss: 8.901262738003624e-11\n",
            "Epoch 844/1000, Loss: 1.358644909632556e-10\n",
            "Epoch 845/1000, Loss: 2.0070325096321896e-10\n",
            "Epoch 846/1000, Loss: 1.6549414527666785e-10\n",
            "Epoch 847/1000, Loss: 1.5955188899241522e-10\n",
            "Epoch 848/1000, Loss: 1.8499140974626015e-10\n",
            "Epoch 849/1000, Loss: 1.4577508286595099e-10\n",
            "Epoch 850/1000, Loss: 1.750196387817482e-10\n",
            "Epoch 851/1000, Loss: 2.5127929361656866e-08\n",
            "Epoch 852/1000, Loss: 4.224537165535702e-05\n",
            "Epoch 853/1000, Loss: 1.5542895314413219e-09\n",
            "Epoch 854/1000, Loss: 1.3758106343075837e-10\n",
            "Epoch 855/1000, Loss: 1.8362404487215488e-10\n",
            "Epoch 856/1000, Loss: 1.1069202054958538e-10\n",
            "Epoch 857/1000, Loss: 1.4330314231092435e-10\n",
            "Epoch 858/1000, Loss: 4.892791341526426e-10\n",
            "Epoch 859/1000, Loss: 1.0444425252054134e-08\n",
            "Epoch 860/1000, Loss: 5.1512565886757676e-09\n",
            "Epoch 861/1000, Loss: 1.239694187211704e-07\n",
            "Epoch 862/1000, Loss: 6.539847838452184e-06\n",
            "Epoch 863/1000, Loss: 2.239708980783517e-07\n",
            "Epoch 864/1000, Loss: 1.5163626492786798e-08\n",
            "Epoch 865/1000, Loss: 5.0997249223414534e-06\n",
            "Epoch 866/1000, Loss: 4.0224432768453336e-05\n",
            "Epoch 867/1000, Loss: 9.949841258960478e-10\n",
            "Epoch 868/1000, Loss: 6.648919330387581e-11\n",
            "Epoch 869/1000, Loss: 3.7275589892526995e-11\n",
            "Epoch 870/1000, Loss: 4.918830810718422e-11\n",
            "Epoch 871/1000, Loss: 1.0421828182394233e-10\n",
            "Epoch 872/1000, Loss: 1.3939611001601015e-10\n",
            "Epoch 873/1000, Loss: 1.544578169809796e-10\n",
            "Epoch 874/1000, Loss: 4.6629740873454023e-11\n",
            "Epoch 875/1000, Loss: 5.118341338868765e-13\n",
            "Epoch 876/1000, Loss: 5.118341338868765e-13\n",
            "Epoch 877/1000, Loss: 5.118341347542382e-13\n",
            "Epoch 878/1000, Loss: 5.11834134645818e-13\n",
            "Epoch 879/1000, Loss: 5.118341341037169e-13\n",
            "Epoch 880/1000, Loss: 5.118341345373978e-13\n",
            "Epoch 881/1000, Loss: 5.118341351879191e-13\n",
            "Epoch 882/1000, Loss: 5.118341338868765e-13\n",
            "Epoch 883/1000, Loss: 5.118341343205574e-13\n",
            "Epoch 884/1000, Loss: 5.118341343205574e-13\n",
            "Epoch 885/1000, Loss: 5.118341341037169e-13\n",
            "Epoch 886/1000, Loss: 5.118341343205574e-13\n",
            "Epoch 887/1000, Loss: 5.118341341037169e-13\n",
            "Epoch 888/1000, Loss: 5.118341349710787e-13\n",
            "Epoch 889/1000, Loss: 5.118341341037169e-13\n",
            "Epoch 890/1000, Loss: 5.118341349710787e-13\n",
            "Epoch 891/1000, Loss: 5.118341347542382e-13\n",
            "Epoch 892/1000, Loss: 5.118341343205574e-13\n",
            "Epoch 893/1000, Loss: 8.744335422850719e-13\n",
            "Epoch 894/1000, Loss: 6.360318181419106e-13\n",
            "Epoch 895/1000, Loss: 6.360318172745488e-13\n",
            "Epoch 896/1000, Loss: 6.718858045883652e-13\n",
            "Epoch 897/1000, Loss: 6.622500364934781e-13\n",
            "Epoch 898/1000, Loss: 6.686853089472866e-13\n",
            "Epoch 899/1000, Loss: 6.6803950609165e-13\n",
            "Epoch 900/1000, Loss: 0.00018230146518388352\n",
            "Epoch 901/1000, Loss: 7.082909901384138e-05\n",
            "Epoch 902/1000, Loss: 3.835038213106934e-10\n",
            "Epoch 903/1000, Loss: 1.265046575271711e-10\n",
            "Epoch 904/1000, Loss: 1.3411914379091172e-10\n",
            "Epoch 905/1000, Loss: 1.2664527526684566e-10\n",
            "Epoch 906/1000, Loss: 1.293916918054805e-10\n",
            "Epoch 907/1000, Loss: 1.4175545100247878e-10\n",
            "Epoch 908/1000, Loss: 1.2682893493765057e-10\n",
            "Epoch 909/1000, Loss: 1.797416644133687e-10\n",
            "Epoch 910/1000, Loss: 1.0826720209244911e-10\n",
            "Epoch 911/1000, Loss: 1.732753666106035e-10\n",
            "Epoch 912/1000, Loss: 1.210565433451549e-10\n",
            "Epoch 913/1000, Loss: 1.2383623487766116e-10\n",
            "Epoch 914/1000, Loss: 1.7645196687121612e-10\n",
            "Epoch 915/1000, Loss: 3.86832248788771e-10\n",
            "Epoch 916/1000, Loss: 1.9301122103354552e-10\n",
            "Epoch 917/1000, Loss: 1.596001941994718e-10\n",
            "Epoch 918/1000, Loss: 1.3878638266406896e-10\n",
            "Epoch 919/1000, Loss: 1.1031145250028906e-10\n",
            "Epoch 920/1000, Loss: 1.7007547636893604e-10\n",
            "Epoch 921/1000, Loss: 1.8265740587705694e-08\n",
            "Epoch 922/1000, Loss: 1.3819041747633598e-06\n",
            "Epoch 923/1000, Loss: 2.419163977357419e-08\n",
            "Epoch 924/1000, Loss: 7.727013605934863e-09\n",
            "Epoch 925/1000, Loss: 2.5100983825118718e-08\n",
            "Epoch 926/1000, Loss: 2.7879824099091622e-05\n",
            "Epoch 927/1000, Loss: 2.9158921315719512e-08\n",
            "Epoch 928/1000, Loss: 1.857012667777491e-09\n",
            "Epoch 929/1000, Loss: 4.97502669116745e-10\n",
            "Epoch 930/1000, Loss: 1.349083990465072e-09\n",
            "Epoch 931/1000, Loss: 4.5284323990646414e-10\n",
            "Epoch 932/1000, Loss: 1.954341913856794e-10\n",
            "Epoch 933/1000, Loss: 1.6776568806387005e-10\n",
            "Epoch 934/1000, Loss: 9.698520514189735e-09\n",
            "Epoch 935/1000, Loss: 1.7492808250102244e-05\n",
            "Epoch 936/1000, Loss: 6.912112520052194e-08\n",
            "Epoch 937/1000, Loss: 7.996540378196748e-10\n",
            "Epoch 938/1000, Loss: 1.5925759017099138e-10\n",
            "Epoch 939/1000, Loss: 2.1334751171275812e-10\n",
            "Epoch 940/1000, Loss: 2.115528409118994e-10\n",
            "Epoch 941/1000, Loss: 1.646943325050998e-10\n",
            "Epoch 942/1000, Loss: 3.936025664452725e-09\n",
            "Epoch 943/1000, Loss: 1.200436259972193e-05\n",
            "Epoch 944/1000, Loss: 1.4153340431424243e-06\n",
            "Epoch 945/1000, Loss: 1.670628684280473e-10\n",
            "Epoch 946/1000, Loss: 1.7135214536168064e-10\n",
            "Epoch 947/1000, Loss: 1.5379096524048563e-10\n",
            "Epoch 948/1000, Loss: 2.0750574253325382e-10\n",
            "Epoch 949/1000, Loss: 2.7203286764865274e-09\n",
            "Epoch 950/1000, Loss: 5.393075433536287e-05\n",
            "Epoch 951/1000, Loss: 1.5716364343176802e-07\n",
            "Epoch 952/1000, Loss: 7.077324807869268e-10\n",
            "Epoch 953/1000, Loss: 1.9894532188313719e-10\n",
            "Epoch 954/1000, Loss: 1.4920781621352574e-10\n",
            "Epoch 955/1000, Loss: 1.1362260379788047e-10\n",
            "Epoch 956/1000, Loss: 1.7851828709725126e-10\n",
            "Epoch 957/1000, Loss: 1.411031242959382e-10\n",
            "Epoch 958/1000, Loss: 1.4926349783950243e-10\n",
            "Epoch 959/1000, Loss: 1.8878823676571343e-10\n",
            "Epoch 960/1000, Loss: 1.812083273433629e-10\n",
            "Epoch 961/1000, Loss: 1.7189423845431762e-10\n",
            "Epoch 962/1000, Loss: 1.7605381694152379e-10\n",
            "Epoch 963/1000, Loss: 1.5571630369859424e-08\n",
            "Epoch 964/1000, Loss: 1.9281563984601126e-06\n",
            "Epoch 965/1000, Loss: 7.944929984989279e-05\n",
            "Epoch 966/1000, Loss: 9.946213063932729e-08\n",
            "Epoch 967/1000, Loss: 2.9919767741137805e-10\n",
            "Epoch 968/1000, Loss: 1.803991475923805e-10\n",
            "Epoch 969/1000, Loss: 1.4195926892335285e-10\n",
            "Epoch 970/1000, Loss: 1.3648089124129735e-10\n",
            "Epoch 971/1000, Loss: 1.4585809510712922e-11\n",
            "Epoch 972/1000, Loss: 2.585566245480453e-12\n",
            "Epoch 973/1000, Loss: 2.7861048563712633e-12\n",
            "Epoch 974/1000, Loss: 3.042268976669038e-12\n",
            "Epoch 975/1000, Loss: 3.1941036817209615e-11\n",
            "Epoch 976/1000, Loss: 1.0132588830480405e-11\n",
            "Epoch 977/1000, Loss: 7.210314067682134e-11\n",
            "Epoch 978/1000, Loss: 1.6775090258547286e-10\n",
            "Epoch 979/1000, Loss: 9.425728422080005e-11\n",
            "Epoch 980/1000, Loss: 1.0741030157055548e-07\n",
            "Epoch 981/1000, Loss: 6.468828089745138e-09\n",
            "Epoch 982/1000, Loss: 3.359131868558562e-05\n",
            "Epoch 983/1000, Loss: 4.6284550197561724e-07\n",
            "Epoch 984/1000, Loss: 1.4720991288463826e-10\n",
            "Epoch 985/1000, Loss: 2.469650794006961e-10\n",
            "Epoch 986/1000, Loss: 2.5739201042762615e-10\n",
            "Epoch 987/1000, Loss: 1.59874589150677e-10\n",
            "Epoch 988/1000, Loss: 6.860819377896288e-11\n",
            "Epoch 989/1000, Loss: 1.0567057869004248e-09\n",
            "Epoch 990/1000, Loss: 4.122767638412017e-08\n",
            "Epoch 991/1000, Loss: 8.665051808187662e-09\n",
            "Epoch 992/1000, Loss: 6.543283510626608e-08\n",
            "Epoch 993/1000, Loss: 4.524876570835778e-06\n",
            "Epoch 994/1000, Loss: 2.3517574903975102e-07\n",
            "Epoch 995/1000, Loss: 1.5747509781852463e-05\n",
            "Epoch 996/1000, Loss: 2.8489855654896255e-06\n",
            "Epoch 997/1000, Loss: 1.0601023265333077e-09\n",
            "Epoch 998/1000, Loss: 8.01576079789701e-10\n",
            "Epoch 999/1000, Loss: 1.7821351400160168e-10\n",
            "Epoch 1000/1000, Loss: 1.7027546117542869e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newNet.filt1.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2bWXa5bvraI",
        "outputId": "2a014296-fede-4d77-85c6-1eca0b50f768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[0.0625, 0.0625, 0.0625],\n",
            "          [0.0625, 0.0625, 0.0625],\n",
            "          [0.0625, 0.0625, 0.0625]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newNet.filt2.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyCGacDnv0uy",
        "outputId": "62471912-2e0b-45f0-d234-b97de0fd5fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[0.1250, 0.2500, 0.1250],\n",
            "          [0.2500, 0.5000, 0.2500],\n",
            "          [0.1250, 0.2500, 0.1250]]]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newNet.filt3.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjUf8a5Gv4ok",
        "outputId": "ff8b0f2d-8957-4cd0-97a5-38d2d4be4a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.0000, -0.5000,  0.0000],\n",
            "          [-0.5000,  0.5000,  0.5000],\n",
            "          [ 0.0000,  0.5000,  1.0000]]]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}